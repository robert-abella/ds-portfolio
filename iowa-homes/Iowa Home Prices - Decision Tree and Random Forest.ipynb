{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries we will use.\n",
    "\n",
    "- **Pandas** - for storing, exploring and manipulating our data in a DataFrame. \n",
    "- **scikit-learn** (sklearn) - for regression models (Decision Tree and Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries: pandas, scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iowa homes prices data is stored in a csv file specified in the path then we load the data to a pandas DataFrame type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: home prices in Iowa\n",
    "iowa_file_path = \"./data/iowa_home.csv\"\n",
    "\n",
    "# Read csv file and load it as a DataFrame in pandas\n",
    "iowa_data = pd.read_csv(iowa_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring our data**\n",
    "\n",
    "Now that we have our dataset stored in a DataFrame, we can now analyze and view summary statistics of our data. We'll use the following:\n",
    "- DataFrame.shape\n",
    "- DataFrame.describe()\n",
    "- DataFrame.head() \n",
    "\n",
    "\n",
    "From our output, we're dealing with 1460 data of Iowa Homes with its features and price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
      "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
      "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
      "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
      "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
      "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
      "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
      "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
      "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
      "\n",
      "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
      "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
      "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
      "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
      "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
      "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
      "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
      "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
      "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
      "\n",
      "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
      "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
      "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
      "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
      "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
      "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
      "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
      "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
      "\n",
      "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
      "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
      "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
      "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
      "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
      "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
      "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
      "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
      "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print a summary statistics of data\n",
    "print(iowa_data.shape)\n",
    "print(iowa_data.describe())\n",
    "#print(iowa_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "#iowa_data.hist(column=[\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning** \n",
    "\n",
    "Since we have 81 variables/columns of Iowa data consisting of features and our target (Price), we will just choose the numeric types that we consider as a good feature of a price.  \n",
    "\n",
    "After checking for NaN values in our dataset, we can see that the features: Alley, PoolQC, Fence and MiscFeature have too many NaNs. It will be better to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in our features\n",
    "null_columns=iowa_data.columns[iowa_data.isnull().any()]\n",
    "iowa_data[null_columns].isnull().sum()\n",
    "\n",
    "\n",
    "# Selecting prediction target using dot notation in pandas\n",
    "iowa_target = iowa_data.SalePrice\n",
    "\n",
    "# First, we drop the target column to have DataFrame of features\n",
    "features = iowa_data.drop(['SalePrice'], axis=1)\n",
    "\n",
    "to_drop = ['Alley', 'PoolQC', 'Fence', 'MiscFeature'] # drop columns because of many NaNs\n",
    "# For this project, we will only include numeric features.\n",
    "features = features.drop(to_drop, axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "# possible features\n",
    "# numeric: LotFrontage, LotArea, YearBuilt, MasVnrArea, GarageArea,\n",
    "#\"1stFlrSF\", \"2ndFlrSF\", \"FullBath\", \"BedroomAbvGr\", \"TotRmsAbvGrd\"\n",
    "\n",
    "iowa_features = ['LotFrontage', 'LotArea', 'YearBuilt', 'MasVnrArea', 'GarageArea',\n",
    "                 '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "features = features[iowa_features]\n",
    "\n",
    "# change NA values in Lot Frontage as 0\n",
    "values = {'LotFrontage': 0, 'MasVnrArea': 0}\n",
    "features = features.fillna(value=values)\n",
    "#features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training and validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    iowa_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling** <br>\n",
    "We first explore Decision Tree then we'll check Random Forest.\n",
    "\n",
    "**Model Validation** <br>\n",
    "Mean Absolute Error (MAE) - one of the metrics for summarizing model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train model using training data\n",
    "# Specify a number for random_state to ensure same results each run\n",
    "iowa_model = DecisionTreeRegressor(random_state=0)\n",
    "iowa_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using validation/test data\n",
    "predictions = iowa_model.predict(X_test)\n",
    "\n",
    "# Model validation: measure the quality of models (Predictive Accuracy)\n",
    "print(\"On average, our predictions are off by about %.2f by using a Decision Tree\" %(mean_absolute_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concepts of underfitting and overfitting\n",
    "\n",
    "# compare MAE with differing values of max_leaf_nodes\n",
    "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n",
    "scores = {leaf_size: get_mae(leaf_size, X_train, X_test, y_train, y_test) for leaf_size in candidate_max_leaf_nodes}\n",
    "#print(scores)\n",
    "\n",
    "# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\n",
    "best_tree_size = min(scores, key=scores.get)\n",
    "print(\"Best tree size (value of max_leaf_nodes) is %d with MAE score of %.2f\" %(best_tree_size, scores[best_tree_size]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    # Help compare mae scores\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Random Forest **\n",
    "\n",
    "Let's check a new model called Random Forest and if it will provide a better predictive accuracy than Decision Trees.\n",
    "\n",
    "*Overview:* The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters.\n",
    "\n",
    "*Parameter Tuning:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - ML model\n",
    "forest_model = RandomForestRegressor(random_state=0, n_estimators=10000)\n",
    "forest_model.fit(X_train, y_train)\n",
    "iowa_preds = forest_model.predict(X_test)\n",
    "print(\"On average, our predictions are off by about %.2f by using Random Forest\" %(mean_absolute_error(y_test, iowa_preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
