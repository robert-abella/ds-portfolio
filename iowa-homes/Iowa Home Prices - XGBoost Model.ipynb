{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** XGBoost **\n",
    "- leading model for working with standard tabular data (ex. Pandas DataFrames)\n",
    "- require more knowledge and model tuning to reach peak accuracy\n",
    "- implementation of the Gradient Boosted Decision Trees algorithm\n",
    "\n",
    "** Gradient Boosted Decision Trees **\n",
    "-  go through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by calculating the errors for each observation in the dataset. We then build a new model to predict those. We add predictions from this error-predicting model to the \"ensemble of models.\"\n",
    "- to make a prediction, we add the predictions from all previous models. We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.\n",
    "- need some base prediction to start the cycle.\n",
    "\n",
    "[Install XGBoost](https://anaconda.org/anaconda/py-xgboost) - conda install -c anaconda py-xgboost (for windows)\n",
    "\n",
    "Note: Research more about XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries: pandas, scikit-learn, xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "#from sklearn.preprocessing import Imputer   -deprecated. use SimpleImputer()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset: Home prices in iowa\n",
    "iowa_file_path = \"./data/iowa_home.csv\"\n",
    "# read csv file and load it as a DataFrame in pandas\n",
    "iowa_data = pd.read_csv(iowa_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data/ Data cleaning/ Data exploration\n",
    "#iowa_data.head()\n",
    "#iowa_data.tail()\n",
    "#iowa_data['TotRmsAbvGrd'].dtypes\n",
    "#df = iowa_data.select_dtypes(include= np.number)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "iowa_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = iowa_data.SalePrice\n",
    "X = iowa_data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "\n",
    "# Split data to training and validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model Tuning **\n",
    "\n",
    "Parameters that can dramatically affect your model's accuracy and training speed:\n",
    "1. n_estimators - specifies how many times to go through the modeling cycle\n",
    "2. early_stopping_rounds - offers a way to automatically find the ideal value\n",
    "3. learning_rate - instead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n",
    "4. n_jobs - On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-19841.19314024 -16332.79042203 -18468.36485141 -17090.28412224\n",
      " -14658.24036841]\n",
      "Mean Absolute Error 17278.17\n"
     ]
    }
   ],
   "source": [
    "# create pipeline\n",
    "#xgb_pipeline = make_pipeline(SimpleImputer(), XGBRegressor(n_estimators=1000, learning_rate=0.05))\n",
    "\n",
    "# New way to create pipeline\n",
    "xgb_pipeline = Pipeline([('imputer', SimpleImputer()), ('xgbrg', XGBRegressor(n_estimators=1000, learning_rate=0.05))])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "fit_params = {'xgbrg__verbose' : False,\n",
    "              'xgbrg__early_stopping_rounds' : 5,\n",
    "              'xgbrg__eval_set' :[(X_test.values, y_test)]\n",
    "             }\n",
    "\n",
    "scores = cross_val_score(xgb_pipeline, X_train.values, y_train, scoring='neg_mean_absolute_error', cv=5, fit_params=fit_params)\n",
    "print(scores)\n",
    "print('Mean Absolute Error %.2f' %(-1 * scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Partial Dependence Plots **\n",
    "- show how each variable or predictor affects the model's predictions.\n",
    "- can be interepreted similarly to the coefficients in those models. But partial dependence plots can capture more complex patterns from your data, and they can be used with any model.\n",
    "- The partial dependence plot is calculated only after the model has been fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = ['LotArea', 'YearBuilt']\n",
    "X = X[cols_to_use]\n",
    "model = GradientBoostingRegressor()\n",
    "gbr_pipeline = make_pipeline(SimpleImputer(),model)\n",
    "gbr_pipeline.fit(X, y)\n",
    "my_plots = plot_partial_dependence(model, \n",
    "                                   features=[0,1], \n",
    "                                   X=X, \n",
    "                                   feature_names=cols_to_use, \n",
    "                                   grid_resolution=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_model = GradientBoostingRegressor()\n",
    "gbr_pipeline = make_pipeline(SimpleImputer(),gbr_model)\n",
    "\n",
    "scores = cross_val_score(gbr_pipeline, X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "print(scores)\n",
    "print('Mean Absolute Error %.2f' %(-1 * scores.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
